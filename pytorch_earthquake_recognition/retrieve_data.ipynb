{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Locate a Station Near The Event"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variable Constants "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:51:57.237522Z",
     "start_time": "2018-02-21T10:51:57.200805Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core.utcdatetime import UTCDateTime\n",
    "from types import MappingProxyType\n",
    "\n",
    "def Day(num):\n",
    "    return 60*60*24*num\n",
    "\n",
    "def get_event_info(name):\n",
    "    \"\"\"\n",
    "    Provides a dictionary (MappingProxyType) of information to populate the global variables needed in the file.\n",
    "    The chosen event is in a seismologically active area. A station is chosen near the event to get data.\n",
    "    @param name: Can be one of - \"Amatrice\", \"Patitirion\"\n",
    "    @return: ProxyMapType, containing the information about the event.\n",
    "    \"\"\"\n",
    "    info: MappingProxyType = None\n",
    "    if name == 'Amatrice':\n",
    "        info = dict(\n",
    "            Latitude='42.790',\n",
    "            Longitude='13.150',\n",
    "            Time=\"2016-08-24 03:36:32\",\n",
    "            StartTime=\"2012-08-24\",\n",
    "            EndTime=\"2018-08-24\",\n",
    "            Client=\"INGV\",\n",
    "            Name=\"Amatrice\",\n",
    "            StationRadius=.2,\n",
    "            LocalEventRadius=1\n",
    "        )\n",
    "\n",
    "    elif name == 'Oklahoma':\n",
    "        info = dict(\n",
    "            Latitude='37.081',\n",
    "            Longitude='-97.309',\n",
    "            Time=\"2018-5-15 20:14:49\",\n",
    "            StartTime=\"2015-10-15\",\n",
    "            EndTime=\"2017-10-15\",\n",
    "            Client=\"IRIS\",\n",
    "            Name=\"Oklahoma\",\n",
    "            StationRadius=1,\n",
    "            LocalEventRadius=1\n",
    "        )\n",
    "    \n",
    "    elif name == \"SouthAmerica\":\n",
    "        info = dict(\n",
    "            Latitude='-25.3',\n",
    "            Longitude='-71',\n",
    "            Time=\"2018-02-20 23:18:32\",\n",
    "            StartTime=\"2013-10-15\",\n",
    "            EndTime=\"2018-01-15\",\n",
    "            Client=\"IRIS\",\n",
    "            Name=\"SouthAmerica\",\n",
    "            StationRadius=3,\n",
    "            LocalEventRadius=1\n",
    "        )\n",
    "    \n",
    "    elif name == \"California\":\n",
    "        info = dict(\n",
    "            Latitude='33.557',\n",
    "            Longitude='-115.888',\n",
    "            Time=\"2018-02-20 23:18:32\",\n",
    "            StartTime=\"2016-10-15\",\n",
    "            EndTime=\"2018-01-15\",\n",
    "            Client=\"IRIS\",\n",
    "            Name=\"California\",\n",
    "            StationRadius=1,\n",
    "            LocalEventRadius=1\n",
    "        )\n",
    "    \n",
    "    elif name == 'Alaska':\n",
    "        info = dict(\n",
    "            Latitude='63.134',\n",
    "            Longitude='-151.115',\n",
    "            Time=\"2018-02-20 23:18:32\",\n",
    "            StartTime=\"2010-01-01\",\n",
    "            EndTime=\"2017-02-26\",\n",
    "            Client=\"IRIS\",\n",
    "            Network=\"NP\",\n",
    "            Station=\"8040\",\n",
    "            Name=\"Alaska\",\n",
    "            StationRadius=6,\n",
    "            LocalEventRadius=1.5\n",
    "        )\n",
    "    \n",
    "    if info: \n",
    "        return MappingProxyType(info)\n",
    "    raise ValueError(f\"Name '{name}' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:51:58.090661Z",
     "start_time": "2018-02-21T10:51:58.087924Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CHANGE THIS VARIABLE TO CHANGE EVERYTHING IN THE FILE\n",
    "event_name = 'Alaska'  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:52:07.354710Z",
     "start_time": "2018-02-21T10:52:07.339370Z"
    },
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieving the quake information            \n",
    "event = get_event_info(event_name)   # dict containing the information to populate the variables\n",
    "\n",
    "# Constants\n",
    "LATITUDE = event['Latitude']                # Latitude of Event\n",
    "LONGITUDE = event['Longitude']              # Longitude of Event\n",
    "STARTTIME = UTCDateTime(event['StartTime']) # Starttime of Data Collection\n",
    "ENDTIME = UTCDateTime(event['EndTime'])     # Endtime of Data Collection\n",
    "CLIENT_NAME = event['Client']               # Client to retrieve event from\n",
    "NAME = event['Name'] + 'Quakes'             # Folder name to write spectrograms to\n",
    "\n",
    "STATION_MAX_RADIUS = event['StationRadius'] # Pick as station within this distance of the event\n",
    "MAX_RADIUS = event['LocalEventRadius']      # How far local events can be from the station\n",
    "NONLOCAL_MIN_RADIUS = 6                     # Min Radius for Nonlocal Events\n",
    "NUM_EVENTS = 2000                            # How many events to retrieve and write                     # Event time window duration\n",
    "NUM_NOISE_EVENTS = NUM_EVENTS\n",
    "DURATION = 15          \n",
    "\n",
    "NETWORK = event.get('Network')\n",
    "STATION = event.get('Station')\n",
    "\n",
    "# Filtering Constants for the Spectrograms\n",
    "MIN_FREQ = 3\n",
    "MAX_FREQ = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Nearest Station "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:52:08.996445Z",
     "start_time": "2018-02-21T10:52:07.773750Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from obspy.core.inventory import Inventory, Station, Network\n",
    "\n",
    "# Types\n",
    "inventory: Inventory\n",
    "network: Network\n",
    "station: Station\n",
    "    \n",
    "\n",
    "# Code\n",
    "client = Client(CLIENT_NAME)\n",
    "\n",
    "inventory = client.get_stations(startbefore=STARTTIME, \n",
    "                               endafter=ENDTIME,\n",
    "                               latitude=LATITUDE,\n",
    "                               longitude=LONGITUDE,\n",
    "                               maxradius=STATION_MAX_RADIUS\n",
    "                              )\n",
    "\n",
    "network = (inventory.select(NETWORK) if NETWORK else inventory)[0]\n",
    "\n",
    "station = (network.select(STATION) if STATION else network)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that waveform FDSN works, so you don't waste time getting to the bottom \n",
    "def get_channel_names(stream):\n",
    "    \"\"\" Returns a list of names of each component, in the order they appear in the stream \"\"\"\n",
    "    return [trace.stats.channel for trace in stream]\n",
    "\n",
    "def verify_fsdn(network, station):\n",
    "    \"\"\" Makes a call to the server \"\"\"\n",
    "    client = Client(CLIENT_NAME)\n",
    "    client.get_waveforms(network.code, station.code, \"*\", \"HN*\", STARTTIME, STARTTIME + DURATION)\n",
    "    return True\n",
    "\n",
    "def get_valid_stations(inventory):\n",
    "    \"\"\" Attempts to get waveforms from a station, with the veriy_fsdn function. Returns valid stations \"\"\"\n",
    "    valid = []\n",
    "    for network in inventory:\n",
    "        for station in network:\n",
    "            try:\n",
    "                verify_fsdn(network, station)\n",
    "            except Exception:\n",
    "                print(\"Failed station:\", station.code)\n",
    "            else:\n",
    "                print(\"Succeeded station:\", station.code)\n",
    "                valid.append(station)\n",
    "    return valid\n",
    "                \n",
    "assert verify_fsdn(network, station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:52:10.442534Z",
     "start_time": "2018-02-21T10:52:10.438631Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Station 8040 (AK:Anchorage;R B Atwood Bld)\n",
      "\tStation Code: 8040\n",
      "\tChannel Count: 0/106 (Selected/Total)\n",
      "\t2003-09-05T18:00:00.000000Z - 2599-12-31T23:59:59.000000Z\n",
      "\tAccess: open \n",
      "\tLatitude: 61.21, Longitude: -149.89, Elevation: -17.4 m\n",
      "\tAvailable Channels:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Optional, Visualize the Station\n",
    "print(station)\n",
    "# inventory.plot(projection='local')\n",
    "# inventory.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Retrieve Local and Non-Local Events From The Station "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for getting the noise times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:52:10.590006Z",
     "start_time": "2018-02-21T10:52:10.484947Z"
    },
    "code_folding": [
     10,
     21,
     29,
     45
    ],
    "collapsed": true,
    "run_control": {
     "marked": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "from bisect import bisect\n",
    "from collections import namedtuple\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Holds a UTCDateTime start and end times of an event\n",
    "Time = namedtuple('Time', ('start', 'end'))   \n",
    "\n",
    "def find_closest_index(a, x):\n",
    "    \"\"\"\n",
    "    Find index of rightmost value less than x via binary search\n",
    "    @param a: Indexable object, the list to look through - MUST BE SORTED\n",
    "    @param x: Any, the item to find the closest index for in a\n",
    "    \"\"\"\n",
    "    i = bisect(a, x) \n",
    "    if i:\n",
    "        return i\n",
    "    raise ValueError\n",
    "\n",
    "def overlaps(time: Time, time2: Time):\n",
    "    \"\"\"\n",
    "    Determines whether two times overlap.\n",
    "    @returns: boolean, True if the time1 overlaps with time2  \n",
    "    \"\"\"\n",
    "    # Starts after the second time ends, or ends before the second time starts\n",
    "    return not ((time.start > time2.end) or (time.end < time2.start))\n",
    "\n",
    "def encompassed(time: Time, sorted_times: List[Time]):\n",
    "    \"\"\"\n",
    "    Determines whether time overlaps with any times in sorted_times\n",
    "    @param time: Time\n",
    "    @param sorted_times: sorted list of Time - the list MUST be sorted for this to work\n",
    "    @return: boolean, True if the time overlaps\n",
    "    \"\"\"\n",
    "    index = find_closest_index(sorted_times, time)\n",
    "    try:\n",
    "        left = overlaps(time, sorted_times[index-1])\n",
    "        curr = overlaps(time, sorted_times[index])\n",
    "        right = overlaps(time, sorted_times[index+1])\n",
    "        return any((left, curr, right))\n",
    "    except IndexError:\n",
    "        return True     \n",
    "\n",
    "def get_noise_times(times_to_exclude: List[UTCDateTime], \n",
    "                    startafter: UTCDateTime, endbefore: UTCDateTime, amount: int, duration: float):\n",
    "    \"\"\"\n",
    "    Generates a list of noise times, which are not in the times_to_exclude\n",
    "    @param times_to_exclude: a list of times of events \n",
    "    @param startafter: Generate noise windows after this time\n",
    "    @param endbefore: Generate noise windows before this time\n",
    "    @param amount: How many noise windows to generate\n",
    "    @param duration: How long each event in times_to_exclude and noise window should be, to prevent overlapping\n",
    "    \"\"\"\n",
    "    exclude = [Time(time, time + duration) for time in times_to_exclude]\n",
    "    exclude.sort()\n",
    "    noise_times = []\n",
    "    \n",
    "    def random_time():\n",
    "        return startafter + random.randint(0, math.floor(endbefore - startafter))\n",
    "    \n",
    "    # Generate a random time\n",
    "    while len(noise_times) < amount:\n",
    "        rt = random_time()\n",
    "        rand_time = Time(rt, rt + duration)\n",
    "                         \n",
    "        if not encompassed(rand_time, exclude):\n",
    "            noise_times.append(rt)\n",
    "            index = bisect(exclude, rand_time)\n",
    "            exclude = exclude[:index] + [rand_time] + exclude[index:]\n",
    "        \n",
    "    return noise_times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:00.530107Z",
     "start_time": "2018-02-21T10:52:10.632519Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL EVENTS: 2000\n",
      "NONLOCAL EVENTS: 2000\n"
     ]
    }
   ],
   "source": [
    "from obspy.clients.fdsn import Client\n",
    "from obspy.core.event import Catalog\n",
    "\n",
    "\n",
    "# Types\n",
    "local_catalog_events: Catalog\n",
    "nonlocal_catalog_events: Catalog\n",
    "    \n",
    "\n",
    "# Code\n",
    "client = Client(CLIENT_NAME)\n",
    "\n",
    "# Local Events\n",
    "local_catalog_events = client.get_events(latitude=station.latitude, \n",
    "                                 longitude=station.longitude,\n",
    "                                 maxradius=MAX_RADIUS,  # Local\n",
    "                                 starttime=STARTTIME,\n",
    "                                 limit=NUM_EVENTS,\n",
    "                                 endtime=ENDTIME\n",
    "                                 )\n",
    "\n",
    "# Nonlocal Events\n",
    "nonlocal_catalog_events = client.get_events(latitude=station.latitude, \n",
    "                                 longitude=station.longitude,\n",
    "                                 minradius=NONLOCAL_MIN_RADIUS,  # Nonlocal\n",
    "                                 limit=NUM_EVENTS,\n",
    "                                 starttime=STARTTIME,\n",
    "                                 endtime=ENDTIME\n",
    "                                 )\n",
    "\n",
    "print(\"LOCAL EVENTS:\", len(local_catalog_events))\n",
    "print(\"NONLOCAL EVENTS:\", len(nonlocal_catalog_events))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:02.202851Z",
     "start_time": "2018-02-21T10:53:02.162271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL EVENTS: 2000\n",
      "NONLOCAL EVENTS: 2000\n"
     ]
    }
   ],
   "source": [
    "local_catalog = local_catalog_events[:NUM_EVENTS]  \n",
    "nonlocal_catalog = nonlocal_catalog_events[:NUM_EVENTS]\n",
    "\n",
    "print(\"LOCAL EVENTS:\", len(local_catalog))\n",
    "print(\"NONLOCAL EVENTS:\", len(nonlocal_catalog))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Events Local and NonLocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:44.363928Z",
     "start_time": "2018-02-21T10:53:44.360999Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Local Events\n",
    "#local_catalog.plot()\n",
    "#local_catalog.plot(projection=\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:44.656946Z",
     "start_time": "2018-02-21T10:53:44.654291Z"
    },
    "code_folding": [],
    "collapsed": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Nonlocal Events\n",
    "# nonlocal_catalog.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Retrieve Waveforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get times where events occur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:45.678542Z",
     "start_time": "2018-02-21T10:53:45.526660Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got times\n"
     ]
    }
   ],
   "source": [
    "from obspy.core import Stream\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "# Functions\n",
    "def get_event_times(catalog: Catalog) -> List[UTCDateTime]:\n",
    "    \"\"\"\n",
    "    Gets event times from a catalog and returns a list of them.\n",
    "    \"\"\"\n",
    "    return [event.origins[0].time for event in catalog]\n",
    "\n",
    "\n",
    "# Types\n",
    "local_times: List[UTCDateTime]\n",
    "non_local_times: List[UTCDateTime]\n",
    "noise_times: List[UTCDateTime]\n",
    "    \n",
    "    \n",
    "# Code\n",
    "catalog_local_times = get_event_times(local_catalog_events)\n",
    "catalog_nonlocal_times = get_event_times(nonlocal_catalog_events)\n",
    "\n",
    "\n",
    "# Get times for each category\n",
    "noise_times = get_noise_times(catalog_local_times + catalog_nonlocal_times, \n",
    "                              min(catalog_nonlocal_times), # startafter\n",
    "                              max(catalog_nonlocal_times), # endbefore\n",
    "                              NUM_NOISE_EVENTS, \n",
    "                              DURATION)\n",
    "\n",
    "local_times =  get_event_times(local_catalog)\n",
    "nonlocal_times = get_event_times(nonlocal_catalog)\n",
    "\n",
    "print(\"Got times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for the client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:46.919907Z",
     "start_time": "2018-02-21T10:53:46.902085Z"
    },
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Bulks\n"
     ]
    }
   ],
   "source": [
    "# Functions\n",
    "end = lambda start_time: start_time + DURATION\n",
    "\n",
    "def create_bulk(times):\n",
    "    return [(network.code, station.code, \"*\", \"HN*\", time, end(time)) for time in times]\n",
    "\n",
    "\n",
    "# Code\n",
    "local_bulk = create_bulk(local_times)\n",
    "nonlocal_bulk = create_bulk(nonlocal_times)\n",
    "noise_bulk = create_bulk(noise_times)\n",
    "print(\"Created Bulks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Waveforms "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Need to do a time test to compare the next two cells "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:48.156724Z",
     "start_time": "2018-02-21T10:53:48.152405Z"
    }
   },
   "outputs": [],
   "source": [
    "##### The non-async way #####\n",
    "\n",
    "# Get Waveforms From Server\n",
    "\n",
    "#waveforms = client.get_waveforms_bulk(bulk) # Can't figure out how to plot this correctly\n",
    "#waveforms = [client.get_waveforms(*data, attach_response=True) for data in local_bulk]\n",
    "\n",
    "# print(\"Got Waveforms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T10:53:49.629765Z",
     "start_time": "2018-02-21T10:53:49.423898Z"
    },
    "code_folding": [
     6,
     30,
     34,
     56
    ],
    "collapsed": true,
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "import asyncio  \n",
    "import time\n",
    "from functools import wraps\n",
    "from obspy.clients.fdsn.header import FDSNNoDataException\n",
    "\n",
    "\n",
    "def slide(iterable, num, stagger=None):\n",
    "    \"\"\" \n",
    "    A sliding window over an iterator, yielding sub arrays.\n",
    "    @param iterable: Iterable\n",
    "    @param num: int, the size of each subarray\n",
    "    @param stagger: int, the number of elements to start the next subarray by \n",
    "    \n",
    "    Example:\n",
    "    for arr in slide(list(range(98)), 5):\n",
    "        print(arr)\n",
    "    \"\"\"\n",
    "    stagger = num - stagger if stagger else 0\n",
    "    start = 0\n",
    "    end = num\n",
    "    \n",
    "    while start + stagger < len(iterable):\n",
    "        yield iterable[start:end]\n",
    "        start = end - stagger\n",
    "        end += num - stagger\n",
    "        \n",
    "        \n",
    "\n",
    "class AsyncClient(Client):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loop = asyncio.get_event_loop()\n",
    "    \n",
    "    async def async_get_waveforms(self, *args, **kwargs):\n",
    "        \"\"\" \n",
    "        Get waveforms asyncronously \n",
    "        @param *args, **kwargs: the arguments passed to Client.get_waveforms\n",
    "        @return: future\n",
    "        \"\"\"\n",
    "        \n",
    "        # kwargs can't be passed to loop.run_in_executor, so create a wrapper function.\n",
    "        @wraps(self.get_waveforms)\n",
    "        def call_with_kwargs(*args):\n",
    "            return self.get_waveforms(*args, **kwargs)\n",
    "            \n",
    "        future = self.loop.run_in_executor(None, call_with_kwargs, *args)\n",
    "        return await future\n",
    "    \n",
    "    def _get_batch(self, batch: List, bulk_kwargs: dict):\n",
    "        \"\"\"\n",
    "        Retrieves a batch of waveforms by calling 'get_waveforms' for each list of args in the batch\n",
    "        @param batch: list[list],each sublist is arguments to be passed to get_waveforms\n",
    "        @param bulk_kwargs: keyword arguments to pass the get_waveforms function. Will be applied to every batch.\n",
    "        \"\"\"\n",
    "        tasks = [self.async_get_waveforms(*data, **bulk_kwargs) for data in batch] # List of Asyncio Tasks\n",
    "        results = self.loop.run_until_complete(asyncio.gather(*tasks))             # Blocks until finished \n",
    "        return results\n",
    "    \n",
    "    def get_waveforms_bulk(self, bulk: List[List], batch_size=10, separation=1, bulk_kwargs=None, skip_errors=True):\n",
    "        \"\"\"\n",
    "        Gets waveforms in a bulk asynchronously.\n",
    "        @param bulk: List[List], data to pass get_waveforms like at - https://docs.obspy.org/packages/autogen/obspy.clients.fdsn.client.Client.get_waveforms_bulk.html\n",
    "        @param batch_size: how many simultaneous requests are sent at one. Keep this number < 10 \n",
    "        @param bulk_kwargs: dict, kwargs to pass to each get_waveforms request because you can't store them in the bulk list.\n",
    "        @param skip_errors: boolean, if False raises FSDN error if the server is missing a waveform  \n",
    "        @return: List[Stream], a list of the waveforms\n",
    "        \"\"\"\n",
    "        if batch_size > 10: raise ValueError(\"Batch size too high, could overload server.\")\n",
    "            \n",
    "        waveforms = []\n",
    "        bulk_kwargs = bulk_kwargs or {}\n",
    "        \n",
    "        generate_waveforms = self.yield_waveforms_bulk(bulk, batch_size, separation, bulk_kwargs, skip_errors)\n",
    "        \n",
    "        for i, result in enumerate(generate_waveforms):\n",
    "            waveforms.extend(result)\n",
    "            print(f\"Got batch {i+1}: {len(waveforms)}/{len(bulk)}\", end=\"\\r\")\n",
    "        \n",
    "        print(\"\\nDone.\")\n",
    "        return waveforms\n",
    "    \n",
    "    def yield_waveforms_bulk(self, bulk, batch_size=10, separation=1, bulk_kwargs=None, skip_errors=True):\n",
    "        \"\"\"\n",
    "        Yields a list of streams of size batch_size. Each call to the generator makes another call to the server.\n",
    "        This way you can be more strategic about how you get your data, hold it in memory, etc.\n",
    "        @yield: List[Stream]\n",
    "        \"\"\"\n",
    "        bulk_kwargs = bulk_kwargs or {}\n",
    "\n",
    "        if batch_size > 10: \n",
    "            raise ValueError(\"Batch size too high, could overload server.\")\n",
    "\n",
    "        for i, batch in enumerate(slide(bulk, batch_size)): \n",
    "            try:\n",
    "                yield self._get_batch(batch, bulk_kwargs)\n",
    "                time.sleep(separation)\n",
    "            except FDSNNoDataException as e:\n",
    "                if skip_errors:\n",
    "                    print(f\"Skipping batch {i} FDSNNoDataException\")\n",
    "                    continue\n",
    "                raise e\n",
    "\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T11:15:30.617850Z",
     "start_time": "2018-02-21T10:53:50.185007Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping batch 159 FDSNNoDataException\n",
      "Skipping batch 177 FDSNNoDataException\n",
      "Skipping batch 194 FDSNNoDataException\n",
      "Skipping batch 195 FDSNNoDataException\n",
      "Skipping batch 196 FDSNNoDataException\n",
      "Skipping batch 197 FDSNNoDataException\n",
      "Skipping batch 198 FDSNNoDataException\n",
      "Skipping batch 199 FDSNNoDataException\n",
      "Skipping batch 200 FDSNNoDataException\n",
      "Skipping batch 201 FDSNNoDataException\n",
      "Skipping batch 202 FDSNNoDataException\n",
      "Skipping batch 203 FDSNNoDataException\n",
      "Skipping batch 204 FDSNNoDataException\n",
      "Skipping batch 205 FDSNNoDataException\n",
      "Skipping batch 206 FDSNNoDataException\n",
      "Skipping batch 210 FDSNNoDataException\n",
      "Skipping batch 239 FDSNNoDataException\n",
      "Skipping batch 264 FDSNNoDataException\n",
      "Skipping batch 265 FDSNNoDataException\n",
      "Skipping batch 266 FDSNNoDataException\n",
      "Skipping batch 267 FDSNNoDataException\n",
      "Skipping batch 268 FDSNNoDataException\n",
      "Skipping batch 269 FDSNNoDataException\n",
      "Skipping batch 270 FDSNNoDataException\n",
      "Skipping batch 271 FDSNNoDataException\n",
      "Skipping batch 272 FDSNNoDataException\n",
      "Skipping batch 273 FDSNNoDataException\n",
      "Skipping batch 274 FDSNNoDataException\n",
      "Skipping batch 275 FDSNNoDataException\n",
      "Skipping batch 276 FDSNNoDataException\n",
      "Skipping batch 277 FDSNNoDataException\n",
      "Skipping batch 278 FDSNNoDataException\n",
      "Skipping batch 279 FDSNNoDataException\n",
      "Got batch 367: 1835/2000\n",
      "Done.\n",
      "Got batch 400: 2000/2000\n",
      "Done.\n",
      "Got batch 400: 2000/2000\n",
      "Done.\n",
      "\n",
      "Retrieved Local: 1835\n",
      "\n",
      "Retrieved Nonlocal: 2000\n",
      "\n",
      "Retrieved Noise: 2000\n"
     ]
    }
   ],
   "source": [
    "# Get Waveforms\n",
    "import obspy\n",
    "client = AsyncClient(CLIENT_NAME)\n",
    "\n",
    "local_waveforms = client.get_waveforms_bulk(local_bulk, \n",
    "                                            batch_size=5, \n",
    "                                            bulk_kwargs=dict(attach_response=True),\n",
    "                                            skip_errors=True)\n",
    "\n",
    "\n",
    "nonlocal_waveforms = client.get_waveforms_bulk(nonlocal_bulk, \n",
    "                                               batch_size=5, \n",
    "                                               bulk_kwargs=dict(attach_response=True),\n",
    "                                               skip_errors=True)\n",
    "\n",
    "noise_waveforms = client.get_waveforms_bulk(noise_bulk, \n",
    "                                               batch_size=5, \n",
    "                                               bulk_kwargs=dict(attach_response=True),\n",
    "                                               skip_errors=True)\n",
    "\n",
    "\n",
    "print(\"\\nRetrieved Local:\", len(local_waveforms))\n",
    "print(\"\\nRetrieved Nonlocal:\", len(nonlocal_waveforms))\n",
    "print(\"\\nRetrieved Noise:\", len(noise_waveforms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T11:17:26.309211Z",
     "start_time": "2018-02-21T11:16:29.941833Z"
    },
    "code_folding": [],
    "run_control": {
     "marked": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def filter_waveforms(waveforms: List[Stream]) -> List[Stream]:\n",
    "    \"\"\" \n",
    "    Filters the waveforms (in place) \n",
    "    @return: generator -> Stream\n",
    "    \"\"\"\n",
    "    waveforms = map(lambda stream: stream.remove_response().filter('bandpass',  \n",
    "                                                                   freqmin=MIN_FREQ, \n",
    "                                                                   freqmax=MAX_FREQ)\\\n",
    "                                                            .taper(max_percentage=.5), waveforms) \n",
    "    return waveforms\n",
    "    \n",
    "\n",
    "local_waves = list(filter_waveforms(local_waveforms.copy()))\n",
    "nonlocal_waves = list(filter_waveforms(nonlocal_waveforms.copy()))\n",
    "noise_waves = list(filter_waveforms(noise_waveforms.copy())) \n",
    "print(\"Filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write Events to Spectrogram Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T11:17:43.632951Z",
     "start_time": "2018-02-21T11:17:43.432171Z"
    },
    "code_folding": [
     6,
     12,
     21,
     25,
     57,
     65
    ],
    "run_control": {
     "marked": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from obspy import Stream\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import NullLocator\n",
    "import glob\n",
    "import os\n",
    "\n",
    "def create_directory(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "        \n",
    "        \n",
    "def write_waveforms(waveforms: List[Stream], path):\n",
    "    create_directory(path)\n",
    "    for i, stream in enumerate(waveforms):\n",
    "        stream.write(os.path.join(path, f'{i}.mseed'), format=\"MSEED\")\n",
    "        \n",
    "        \n",
    "def write_waveform(stream, file_path):\n",
    "    print(\"Writing: \", os.path.basename(file_path), end='\\r')\n",
    "    stream.write(file_path, format=\"MSEED\")\n",
    "    \n",
    "\n",
    "def async_write_waveforms(waveforms: List[Stream], path):\n",
    "    print(\"Writing Waveforms...\")\n",
    "    create_directory(path)\n",
    "\n",
    "    work = [] \n",
    "    \n",
    "    # Prep the file paths\n",
    "    for i, stream in enumerate(waveforms):\n",
    "        file_path = os.path.join(path, f'{i}.mseed')\n",
    "        \n",
    "        # Add the args for each stream\n",
    "        work.append(\n",
    "            (stream, file_path)  \n",
    "        )\n",
    "\n",
    "    # Run with multiprocess\n",
    "    pool = Pool()\n",
    "    pool.starmap(write_waveform, work)\n",
    "    \n",
    "    print(\"\\nWrote Waveforms\")\n",
    "    \n",
    "\n",
    "def remove_borders(path):\n",
    "    \"\"\" \n",
    "    Removes borders of all .png files in curr directory  \n",
    "    Must have ImageMagik installed.\n",
    "    \"\"\"\n",
    "    png_paths = glob.glob(f\"{path}/*.png\")\n",
    "    \n",
    "    for file in png_paths:\n",
    "        print(\"Trimming: \", file, end='\\r')\n",
    "        !convert $file -trim +repage $file;\n",
    "        \n",
    "\n",
    "def write_spectrogram(stream, path):\n",
    "    \"\"\"\n",
    "    @param stream: Stream, the file to make a spectrogram from.\n",
    "                   Each trace should be one component.\n",
    "    \n",
    "    @param path: Str, the directory to write into.\n",
    "    \n",
    "    Each component of the stream is written as its own figure, a .png file.\n",
    "    Components eg. \"NNE\" are saved as the filename eg. \"NNE.png\".\n",
    "    \"\"\"\n",
    "    create_directory(path)\n",
    "    figure_list = stream.spectrogram(show=False, title=\"\")\n",
    "    \n",
    "    for figure, channel in zip(figure_list, get_channel_names(stream)):\n",
    "        axes = figure.gca() \n",
    "        axes.set_xlim([0, DURATION])\n",
    "        axes.set_ylim([0, MAX_FREQ])\n",
    "\n",
    "        # Get Rid of Whitespace\n",
    "        axes.axis(\"off\")\n",
    "        axes.xaxis.set_major_locator(NullLocator())\n",
    "        axes.yaxis.set_major_locator(NullLocator())\n",
    "        figure.savefig(f\"{path}/{channel}.png\", bbox_inches='tight', pad_inches=0, transparent=True)\n",
    "        \n",
    "        # Free memory\n",
    "        figure.clf()\n",
    "        plt.close(figure) \n",
    "\n",
    "    remove_borders(path)  # Remove borders of all the spectrograms\n",
    "    \n",
    "    \n",
    "\n",
    "def write_spectrograms(waveforms, path):   \n",
    "    for i, stream in enumerate(waveforms):\n",
    "        dir_path = os.path.join(path, str(i))\n",
    "        create_directory(dir_path)\n",
    "        write_spectrogram(stream, dir_path)\n",
    "    \n",
    "\n",
    "def async_write_spectrograms(waveforms, path):\n",
    "    \"\"\"\n",
    "    @param waveforms: List[Stream], the waveforms to make spectrograms from.\n",
    "    @param path: str, The directory to write to\n",
    "    \n",
    "    1. Calls 'write_spectrogram' on each Stream. \n",
    "    2. Each stream is written to its own folder\n",
    "    3. All components are written to their own file as a spectrogram .png image.\n",
    "    \"\"\"\n",
    "    print(\"Writing Files...\")\n",
    "\n",
    "    work = []   # Args that will be passed to the write_spectrogram function\n",
    "    \n",
    "    # Prep the file paths\n",
    "    for i, stream in enumerate(waveforms):\n",
    "        dir_path = os.path.join(path, str(i))\n",
    "        create_directory(dir_path)\n",
    "        \n",
    "        # Add the args for each stream\n",
    "        work.append(\n",
    "            (stream, os.path.join(path, str(i)))  # arguments to the 'write_spectrogram' function\n",
    "        )\n",
    "\n",
    "    # Run with multiprocess\n",
    "    pool = Pool()\n",
    "    pool.starmap(write_spectrogram, work)             # Map the 'write_spectrogram' function to the work\n",
    "    \n",
    "    print(\"\\nWrote Files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Waveforms...\n",
      "\n",
      "Wrote Waveforms\n",
      "Writing Waveforms...\n",
      "\n",
      "Wrote Waveforms\n",
      "Writing Waveforms...\n",
      "\n",
      "Wrote Waveforms\n",
      "Finished Writing\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "# Constants\n",
    "DIR_PATH = os.path.join(os.getcwd(), f\"waveforms/{NAME}\")\n",
    "\n",
    "# Write Waveforms\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    async_write_waveforms(local_waves, os.path.join(DIR_PATH, \"local\"))   \n",
    "    async_write_waveforms(nonlocal_waves, os.path.join(DIR_PATH, \"non_local\"))   \n",
    "    async_write_waveforms(noise_waves, os.path.join(DIR_PATH, \"noise\"))   \n",
    "    \n",
    "print(\"Finished Writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### READ WAVEFORMS ####\n",
    "\n",
    "# from obspy import read\n",
    "# import glob\n",
    "# import os\n",
    "\n",
    "# DIR_PATH = os.path.join(os.getcwd(), f\"waveforms/{NAME}/noise\")\n",
    "# def read_waveforms(dir_path):\n",
    "#     files = glob.glob(os.path.join(dir_path, '*.mseed'))\n",
    "#     waveforms = [read(file) for file in files]\n",
    "#     return waveforms\n",
    "\n",
    "# noise_waves = read_waveforms(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-21T11:26:37.813472Z",
     "start_time": "2018-02-21T11:17:47.227497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming:  /data/pytorch_earthquake_recognition/spectrograms/AlaskaQuakes/local/0/HNZ.png\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming:  /data/pytorch_earthquake_recognition/spectrograms/AlaskaQuakes/local/1565/HNE.png\n",
      "Wrote Files\n",
      "Writing Files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming:  /data/pytorch_earthquake_recognition/spectrograms/AlaskaQuakes/non_local/63/HNZ.pngg\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming:  /data/pytorch_earthquake_recognition/spectrograms/AlaskaQuakes/non_local/0/HNE.pngng\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/matplotlib/pyplot.py:523: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming:  /data/pytorch_earthquake_recognition/spectrograms/AlaskaQuakes/non_local/319/HNE.png\r"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "DIR_PATH = os.path.join(os.getcwd(), f\"spectrograms/{NAME}\")\n",
    "\n",
    "# Write Waveforms\n",
    "async_write_spectrograms(local_waves, os.path.join(DIR_PATH, \"local\"))   \n",
    "async_write_spectrograms(nonlocal_waves, os.path.join(DIR_PATH, \"non_local\"))   \n",
    "async_write_spectrograms(noise_waves, os.path.join(DIR_PATH, \"noise\"))   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest \n",
    "\n",
    "class TestMethods(unittest.TestCase):\n",
    " \n",
    "\n",
    "    def test_overlaps(self):\n",
    "        time1 = UTCDateTime(2015, 1, 1, 10, 0, 10)\n",
    "        time2 = UTCDateTime(2015, 1, 1, 10, 0, 19)\n",
    "        time3 = UTCDateTime(2015, 1, 1, 10, 0, 30)\n",
    "        t1 = Time(time1, time1 + 10)\n",
    "        t2 = Time(time2, time2 + 10)\n",
    "        t3 = Time(time3, time3 + 10)\n",
    "        self.assertTrue(overlaps(t1, t2))\n",
    "        self.assertFalse(overlaps(t2, t3))\n",
    "\n",
    "        \n",
    "    def test_encompassed(self):\n",
    "        t = UTCDateTime(2015, 1, 1, 10, 0, 0) \n",
    "        times = [Time(t + 10*i, t + 10*(i+1)) for i in range(1, 7)]  # 1 minute of times \n",
    "        times.append(Time(t+1000, t+1010))\n",
    "        times.append(Time(t+1010, t+1020))\n",
    "        times.sort()                           # sorted just to remember that the function requires sorted times\n",
    "        \n",
    "        \n",
    "        # Encompassed within the times\n",
    "        self.assertTrue(encompassed(Time(t+30, t+40), times))\n",
    "        \n",
    "        # Not encompassed\n",
    "        self.assertFalse(encompassed(Time(t+990, t+999), times))\n",
    "        \n",
    "        # Value should be within the range of times... otherwise it is invalid.\n",
    "        # This is because we only have knowledge of events between the start and end time\n",
    "        # we can't generate noise times outside this range.\n",
    "        with self.assertRaises(ValueError):\n",
    "            encompassed(Time(t-10, t), times) \n",
    "            \n",
    "            \n",
    "    def test_find_closest_index(self):\n",
    "        lst = [1, 2, 3, 5, 6]\n",
    "        self.assertEqual(find_closest_index(lst, 2), 2)  # defaults to finding index to the right  \n",
    "        self.assertEqual(find_closest_index(lst, 4), 3) \n",
    "        \n",
    "        with self.assertRaises(ValueError):\n",
    "            find_closest_index(lst, 0)\n",
    "            \n",
    "            \n",
    "            \n",
    "    def test_get_noise_times(self):\n",
    "        t = UTCDateTime(2015, 1, 1, 10, 0, 0) \n",
    "        exclude = [t + 60*i for i in range(1, 11)]  # 10 times in a 10 minute range \n",
    "    \n",
    "        times = get_noise_times(exclude, startafter=exclude[0], endbefore=exclude[-1], amount=10, duration=10)\n",
    "        \n",
    "        \n",
    "        # Make sure none of the noise times overlap with the event (exclude) times\n",
    "        exclude_times = [Time(time, time+10) for time in exclude]\n",
    "        self.assertFalse(any((encompassed(Time(time, time+10), exclude_times)) for time in times))\n",
    "        \n",
    "        # Make sure that the times are generated in the correct time window\n",
    "        times.sort()\n",
    "        self.assertTrue((times[0] > exclude[0]) and (times[-1] < exclude[-1]))\n",
    "        \n",
    "        # Should generate the correct amount\n",
    "        self.assertEqual(len(times), 10)\n",
    "        \n",
    "        \n",
    "    def test_slide(self):\n",
    "        gen = slide(list(range(7)), 3)\n",
    "        self.assertEqual(next(gen), [0, 1, 2])\n",
    "        self.assertEqual(next(gen), [3, 4, 5])\n",
    "        self.assertEqual(next(gen), [6])\n",
    "        \n",
    "        res = list(slide(list(range(7)), 3, 2))\n",
    "        self.assertEqual(res, [[0, 1, 2], [2, 3, 4], [4, 5, 6]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods = TestMethods()\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromModule(methods)\n",
    "unittest.TextTestRunner().run(suite)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 397.566666,
   "position": {
    "height": "474px",
    "left": "926.417px",
    "right": "102.417px",
    "top": "98px",
    "width": "489px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
